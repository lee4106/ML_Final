# -*- coding: utf-8 -*-
"""50024_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k7FZ7g6qe4V8EwYP3Iasa23hSMsScXBX

# **Import the packages**
"""

!pip install higher
import numpy as np
import pandas as pd
import higher
import matplotlib.pyplot as plt
import torch
import torchvision
import torchvision.transforms as transforms
import itertools
from tqdm.notebook import tqdm
from torch import autograd

"""**Fuctions to generate imbalanced train datasets, balanced test dataset and clean dataset**"""

args = {'batch_size':100, 'lr':0.001, 'n_epochs':100, 'device':torch.device("cuda" if torch.cuda.is_available() else "cpu")}

def get_clean_dataset(testset, set_size = 0.002):
  #split cat and dog datasets
  train_cat_idxs = np.where(np.array(testset.targets) == 3)[0] 
  train_dog_idxs = np.where(np.array(testset.targets) == 5)[0] 
  total_cats = len(train_cat_idxs) #5000
  total_dogs = len(train_dog_idxs) #5000
  num_of_cats = int(total_cats*set_size)
  num_of_dogs = int(total_dogs*set_size)
  #clean data set
  clean_idxs = np.concatenate([train_cat_idxs[:num_of_cats], train_dog_idxs[:num_of_dogs]])
  clean_dataset = torch.utils.data.Subset(testset,clean_idxs)
  clean_loader = torch.utils.data.DataLoader(clean_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=2)
  return clean_loader
def get_imbalanced_datasets(trainset,imbalance_ratio):  #ratio = cats/dogs
  #split cat and dog datasets
  train_cat_idxs = np.where(np.array(trainset.targets) == 3)[0] 
  train_dog_idxs = np.where(np.array(trainset.targets) == 5)[0] 
  total_cats = len(train_cat_idxs) #5000
  total_dogs = len(train_dog_idxs) #5000
  #Imbalanced dataset
  num_of_cats = int(total_cats*imbalance_ratio)
  imbl_train_idxs = np.concatenate([train_cat_idxs[:num_of_cats], train_dog_idxs[:total_dogs]])
  imbl_train_dataset = torch.utils.data.Subset(trainset,imbl_train_idxs)
  data_loader = torch.utils.data.DataLoader(imbl_train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=2)
  return data_loader

def get_balanced_datasets(testset):
  test_cat_idxs = np.where(np.array(testset.targets) == 3)[0] 
  test_dog_idxs = np.where(np.array(testset.targets) == 5)[0]   
  total_cats = len(test_cat_idxs) #5000
  total_dogs = len(test_dog_idxs) #5000
  #balanced dataset
  test_idxs = np.concatenate([test_cat_idxs[:total_cats], test_dog_idxs[:total_dogs]])
  test_dataset = torch.utils.data.Subset(testset,test_idxs)
  data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=2)
  return data_loader

"""# **Helper Functions**"""

class LeNet5(torch.nn.Module):          
    def __init__(self):     
        super(LeNet5, self).__init__()
        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)
        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)
        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)
        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2) 
        self.fc1 = torch.nn.Linear(16*6*6, 120)   
        self.fc2 = torch.nn.Linear(120, 84)
        self.fc3 = torch.nn.Linear(84, 6)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.conv1(x))  
        x = self.max_pool_1(x) 
        x = torch.nn.functional.relu(self.conv2(x))
        x = self.max_pool_2(x)
        x = x.view(-1, 16*6*6)
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x).squeeze()
        return x

def accuracy(network, dataloader):
  network.eval()
  total_correct = 0
  total_instances = 0
  for  _, (images, labels) in enumerate(dataloader):
    images, labels = images.to(device = args['device'], non_blocking=True), labels.to(device = args['device'], non_blocking=True)
    predictions = torch.argmax(network(images), dim=1)
    correct_predictions = sum(predictions==labels).item()
    total_correct+=correct_predictions
    total_instances+=len(images)
  return round(total_correct/total_instances, 3)

def train_base_model(model, criterion, optimizer, train_loader):
  loss_track = []
  accuracy_track = []
  for i in tqdm(range(1, args['n_epochs']+1)):
    model.train()
    accu = 0
    loss = 0
    correct = 0
    sum = 0
    for _, (images, labels) in enumerate(train_loader):
      images,labels = images.to(device = args['device'], non_blocking=True),labels.to(device = args['device'], non_blocking=True)
      outputs = model(images)
      optimizer.zero_grad()
      training_loss = criterion(outputs, labels)
      training_loss.backward()
      optimizer.step()
      loss += training_loss.item()*outputs.shape[0]
    loss_track.append(loss/(len(train_loader)*100))
    
    #evaluating
    model.eval()
    with torch.no_grad():
        train_accu = accuracy(model, train_loader)
        accuracy_track.append(train_accu)
    print('Training Loss:', loss/(len(train_loader)*100), 'Training Accuracy:', train_accu)
  return loss_track, accuracy_track

def base_model(model, criterion, optimizer, train_loader, test_loader):
  train_loss_track = []
  train_accuracy_track = []
  test_loss_track = []
  test_accuracy_track = []
  for i in tqdm(range(1, args['n_epochs']+1)):
    model.train()
    tr_accu = 0
    accu = 0
    tr_loss = 0
    loss = 0
    correct = 0
    sum = 0
    for _, (images, labels) in enumerate(train_loader):
      images,labels = images.to(device = args['device'], non_blocking=True),labels.to(device = args['device'], non_blocking=True)
      outputs = model(images)
      optimizer.zero_grad()
      training_loss = criterion(outputs, labels)
      training_loss.backward()
      optimizer.step()
      loss += training_loss.item()*outputs.shape[0]
      _, predicted = torch.max(outputs.data, 1)
      correct += predicted.eq(labels.data).cpu().sum()
    tr_loss = loss/len(train_loader.dataset)
    tr_accu = correct.numpy() / len(train_loader.dataset)
    train_loss_track.append(tr_loss)
    train_accuracy_track.append(tr_accu)
    
    #Test evaluating
    model.eval()
    criterion.reduction = 'mean'
    running_loss = 0.0
    correct = 0
    with torch.no_grad():
        for i, (inputs, targets) in enumerate(test_loader):
            inputs, targets = inputs.to(device = args['device']), targets.to(device = args['device'])
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item() * outputs.shape[0]
            _, predicted = torch.max(outputs.data, 1)
            correct += predicted.eq(targets.data).cpu().sum()
    epoch_loss = running_loss / len(test_loader.dataset)
    epoch_acc = correct.numpy() / len(test_loader.dataset)
    test_loss_track.append(epoch_loss)
    test_accuracy_track.append(epoch_acc)
    print('Training Loss:', tr_loss, 'Training Accuracy:', tr_accu,'Testing Loss:', epoch_loss, 'Testing Accuracy:', epoch_acc)
  return train_loss_track, train_accuracy_track, test_loss_track, test_accuracy_track

def train_model_reweighting(model, criterion, optimizer, train_loader, clean_loader):
    loss_track = []
    accuracy_track = []
    for i in tqdm(range(1, args['n_epochs']+1)):
        model.train()
        loss = 0
        # Step 2: Sample mini-batch from Df
        for _, (images,labels) in enumerate(train_loader):
          images,labels = images.to(device = args['device'], non_blocking=True),labels.to(device = args['device'], non_blocking=True)
          optimizer.zero_grad()
          
          with higher.innerloop_ctx(model, optimizer) as (clean_model, clean_optimizer):
            # Step 1. Update meta model on training data
            ctrain_outputs = clean_model(images)
            criterion.reduction = 'none'
            ctrain_loss = criterion(ctrain_outputs, labels)
            eps = torch.zeros(ctrain_loss.size(), requires_grad=True, device=args['device'])
            ctrain_loss = torch.sum(eps * ctrain_loss)
            clean_optimizer.step(ctrain_loss)

            # Step 2. Compute grads of eps on meta validation data
            clean_images, clean_labels =  next(clean_loader)
            #meta_labels =meta_labels.unsqueeze(1)  
            clean_images, clean_labels = clean_images.to(device=args['device'], non_blocking=True),\
                            clean_labels.to(device=args['device'], non_blocking=True)

            clean_val_outputs = clean_model(clean_images)
            criterion.reduction = 'mean'
            clean_val_loss = criterion(clean_val_outputs, clean_labels)
            eps_grads = torch.autograd.grad(clean_val_loss, eps)[0].detach()
          # Step 3:
          w_hat = torch.clamp(-eps_grads, min = 0)
          sum = torch.sum(w_hat)
          if (sum == 0):
            weight = w_hat
          else:
            weight = w_hat/sum

          # Step 4: Forward pass on Xf with current theta
          y_hat_f = model(images)
          criterion.reduction = 'none'
          mloss = criterion(y_hat_f, labels)
          minib_loss = torch.sum(mloss*weight)

          optimizer.zero_grad()
          minib_loss.backward()
          optimizer.step()

          loss += minib_loss.item()*y_hat_f.shape[0]
        loss_track.append(loss/(len(train_loader)*100))
        model.eval()
        with torch.no_grad():
          train_accu = accuracy(model, train_loader)
          accuracy_track.append(train_accu)
        print('Training Loss:', loss/(len(train_loader)*100), 'Training Accuracy:', train_accu)
    return loss_track, accuracy_track

def model_reweighting(model, criterion, optimizer, train_loader, clean_loader, test_loader):
      train_loss_track = []
      train_accuracy_track = []
      test_loss_track = []
      test_accuracy_track = []
      for i in tqdm(range(1, args['n_epochs']+1)):
        model.train()
        tr_accu = 0
        accu = 0
        tr_loss = 0
        loss = 0
        correct = 0
        sum = 0
        # Step 2: Sample mini-batch from Df
        for _, (images,labels) in enumerate(train_loader):
          images,labels = images.to(device = args['device'], non_blocking=True),labels.to(device = args['device'], non_blocking=True)
          optimizer.zero_grad()
          
          with higher.innerloop_ctx(model, optimizer) as (clean_model, clean_optimizer):
            # Step 1. Update meta model on training data
            ctrain_outputs = clean_model(images)
            criterion.reduction = 'none'
            ctrain_loss = criterion(ctrain_outputs, labels)
            eps = torch.zeros(ctrain_loss.size(), requires_grad=True, device=args['device'])
            ctrain_loss = torch.sum(eps * ctrain_loss)
            clean_optimizer.step(ctrain_loss)

            # Step 2. Compute grads of eps on meta validation data
            clean_images, clean_labels =  next(clean_loader)
            #meta_labels =meta_labels.unsqueeze(1)  
            clean_images, clean_labels = clean_images.to(device=args['device'], non_blocking=True),\
                            clean_labels.to(device=args['device'], non_blocking=True)

            clean_val_outputs = clean_model(clean_images)
            criterion.reduction = 'mean'
            clean_val_loss = criterion(clean_val_outputs, clean_labels)
            eps_grads = torch.autograd.grad(clean_val_loss, eps)[0].detach()
          # Step 3:
          w_hat = torch.clamp(-eps_grads, min = 0)
          sum = torch.sum(w_hat)
          if (sum == 0):
            weight = w_hat
          else:
            weight = w_hat/sum


          y_hat_f = model(images)
          criterion.reduction = 'none'
          mloss = criterion(y_hat_f, labels)
          minib_loss = torch.sum(mloss*weight)

          optimizer.zero_grad()
          minib_loss.backward()
          optimizer.step()

          loss += minib_loss.item()*y_hat_f.shape[0]
          _, predicted = torch.max(y_hat_f.data, 1)
          correct += predicted.eq(labels.data).cpu().sum()
        tr_loss = loss/len(train_loader.dataset)
        tr_accu = correct.numpy() / len(train_loader.dataset)
        train_loss_track.append(tr_loss)
        train_accuracy_track.append(tr_accu)
        
        #Test evaluating
        model.eval()
        criterion.reduction = 'mean'
        running_loss = 0.0
        correct = 0
        with torch.no_grad():
            for i, (inputs, targets) in enumerate(test_loader):
                inputs, targets = inputs.to(device = args['device']), targets.to(device = args['device'])
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                running_loss += loss.item() * outputs.shape[0]
                _, predicted = torch.max(outputs.data, 1)
                correct += predicted.eq(targets.data).cpu().sum()
        epoch_loss = running_loss / len(test_loader.dataset)
        epoch_acc = correct.numpy() / len(test_loader.dataset)
        test_loss_track.append(epoch_loss)
        test_accuracy_track.append(epoch_acc)
        print('Training Loss:', tr_loss, 'Training Accuracy:', tr_accu,'Testing Loss:', epoch_loss, 'Testing Accuracy:', epoch_acc)
      return train_loss_track, train_accuracy_track, test_loss_track, test_accuracy_track

def test_model0(model, criterion, test_loader):
    test_loss_hist = []
    test_accu_hist = []
    total = 0
    for i in tqdm(range(1, args['n_epochs']+1)):
      model.eval()  # Set the model to evaluation mode
      loss = 0

      correct = 0
      with torch.no_grad():
          for  _, (images,labels) in enumerate(test_loader):
            images,labels = images.to(device = args['device']),labels.to(device = args['device'])
            predict = model(images)
            testing_loss = criterion(predict, labels)
            loss += testing_loss.item() * predict.shape[0]
            _, predicted = torch.max(predict.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
      test_loss_hist.append(loss/len(test_loader))
      #test_accu = accuracy(model, test_loader)
      test_accu_hist.append(correct/len(test_loader))

      print('Training Loss:', loss/len(test_loader), 'Training Accuracy:', correct/len(test_loader))
    return test_loss_hist, test_accu_hist

def test_model(model, criterion, test_loader):
  loss_track = []
  accuracy_track = []
  for i in tqdm(range(1, args['n_epochs']+1)):
    model.train()
    accu = 0
    loss = 0
    correct = 0
    sum = 0
    for _, (images, labels) in enumerate(test_loader):
      images,labels = images.to(device = args['device'], non_blocking=True),labels.to(device = args['device'], non_blocking=True)
      outputs = model(images)
      training_loss = criterion(outputs, labels)
      training_loss.backward()
      loss += training_loss.item()*outputs.shape[0]
    loss_track.append(loss/(len(test_loader)*100))
    model.eval()
    with torch.no_grad():
        train_accu = accuracy(model, test_loader)
        accuracy_track.append(train_accu)
    print('Testing Loss:', loss/(len(test_loader)*100), 'Testing Accuracy:', train_accu)
  return loss_track, accuracy_track

"""## **Main**

Laoding the datasets and split it
"""

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = args['batch_size']
#load the train dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

#load train dataset, imbalanced dataset
imbl_train_loader  = get_imbalanced_datasets(trainset, 0.995)

#load the test dataset
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
test_loader = get_balanced_datasets(testset)

"""Training a base model"""

model = LeNet5()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'])

#training baseline
#loss_hist, accu_hist = train_base_model(model, criterion, optimizer, imbl_train_loader)
bs_tr_loss_hist, bs_tr_accu_hist, bs_ts_loss_hist, bs_ts_accu_hist = base_model(model, criterion, optimizer, imbl_train_loader, test_loader)

"""Training a reweight model"""

#traning reweight model
rw_model = LeNet5()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(rw_model.parameters(), lr=args['lr'])
#load train dataset, imbalanced dataset
imbl_train_loader  = get_imbalanced_datasets(trainset, 0.995)

#load the test dataset
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
test_loader = get_balanced_datasets(testset)

#load clean data set
clean_loader = get_clean_dataset(trainset)
clean_loader = itertools.cycle(clean_loader)
#load test data set
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
test_loader = get_balanced_datasets(testset)
#traning
rw_tr_loss_hist, rw_tr_accu_hist, rw_ts_loss_hist, rw_ts_accu_hist = model_reweighting(rw_model, criterion, optimizer, imbl_train_loader, clean_loader, test_loader)

"""Plot the figures"""

#trainning logs--------------------------------------------
# Plot the loss history
rg = np.linspace(1,len(bs_tr_loss_hist),len(bs_tr_loss_hist))
plt.plot(rg, bs_tr_loss_hist, 'r', label='baseline')
plt.plot(rg, rw_tr_loss_hist, 'b', label='model(ratio=0.9)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Logs')
plt.legend()
plt.show()

# Plot the accuracy history
rg = np.linspace(1,len(bs_tr_accu_hist),len(bs_tr_accu_hist))
plt.plot(rg, bs_tr_accu_hist, 'r', label='baseline')
plt.plot(rg, rw_tr_accu_hist, 'b', label='model(ratio=0.9)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training Accuracy Logs')
plt.legend()
plt.show()

#Testing logs----------------------------------------------
# Plot the loss history
rg = np.linspace(1,len(bs_ts_loss_hist),len(bs_ts_loss_hist))
plt.plot(rg, bs_ts_loss_hist, 'r', label='baseline')
plt.plot(rg, rw_ts_loss_hist, 'b', label='model(ratio=0.9)')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.title('Testing Loss Logs')
plt.legend()
plt.show()

# Plot the accuracy history
rg = np.linspace(1,len(bs_ts_accu_hist),len(bs_ts_accu_hist))
plt.plot(rg, bs_ts_accu_hist, 'r', label='baseline')
plt.plot(rg, rw_ts_accu_hist, 'b', label='model(ratio=0.9)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Testing Accuracy Logs')
plt.legend()
plt.show()

"""Train models on different imbalanced ratio dataset"""

args = {'batch_size':100, 'lr':0.006, 'n_epochs':15, 'device':torch.device("cuda" if torch.cuda.is_available() else "cpu")}
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = args['batch_size']
#load the train dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

ratios = [0.9, 0.95, 0.98, 0.99, 0.995]

base_accu = []
rw_accu = []
for r in ratios:
  #load train dataset, imbalanced dataset
  imbl_train_loader  = get_imbalanced_datasets(trainset, r)

  #load the test dataset
  test_loader = get_balanced_datasets(testset)

  #define model
  model = LeNet5()
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'])

  #training baseline
  bs_tr_loss_hist, bs_tr_accu_hist, bs_ts_loss_hist, bs_ts_accu_hist = base_model(model, criterion, optimizer, imbl_train_loader, test_loader)

  base_accu.append(sum(bs_ts_accu_hist)/len(bs_ts_accu_hist))  #store avg base accu

  #train rewright model
  rw_model = LeNet5()
  rw_criterion = torch.nn.CrossEntropyLoss()
  rw_optimizer = torch.optim.SGD(rw_model.parameters(), lr=args['lr'])
  #load clean data set
  clean_loader = get_clean_dataset(trainset)
  clean_loader = itertools.cycle(clean_loader)
  #load test data set
  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
  test_loader = get_balanced_datasets(testset)
  #traning
  rw_tr_loss_hist, rw_tr_accu_hist, rw_ts_loss_hist, rw_ts_accu_hist = model_reweighting(rw_model, rw_criterion, rw_optimizer, imbl_train_loader, clean_loader, test_loader)

  rw_accu.append(sum(rw_ts_accu_hist)/len(rw_ts_accu_hist))  #store avg base accu

"""Plot the figure"""

#plotting the result

plt.plot(ratios, base_accu, 'r', label='baseline')
plt.plot(ratios, rw_accu, 'b', label='our model')
plt.xlabel('Ratio')
plt.ylabel('Accuracy')
plt.title('Testing Accuracy On Different Imbalanced Ratio')
plt.legend()
plt.show()